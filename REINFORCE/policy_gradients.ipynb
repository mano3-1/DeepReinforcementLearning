{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "policy gradients.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "69enXgQ7te1y"
      },
      "source": [
        "import torch as T\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT5RT570tnuQ",
        "outputId": "07b36bb9-52b7-4df8-e58a-36bb72a7b23a"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "env.observation_space.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Vx1qkbtslp"
      },
      "source": [
        "class Policy_Network(nn.Module):\n",
        "    def __init__(self, lr, input_dims, n_actions, gamma=0.999, fc1 = 256, fc2 = 128):\n",
        "        super(Policy_Network,self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.n_actions = n_actions\n",
        "       \n",
        "        self.fc1 = fc1 \n",
        "        self.fc2 = fc2\n",
        "        \n",
        "        self.fc1_l = nn.Linear(*self.input_dims,self.fc1)\n",
        "        self.fc2_l = nn.Linear(self.fc1,self.fc2)\n",
        "        self.final = nn.Linear(self.fc2,self.n_actions)\n",
        "        \n",
        "        self.device = 'cuda:0' if T.cuda.is_available() else 'cpu'\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr = lr)\n",
        "        self.to(self.device)\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def forward(self,X):\n",
        "        X = X.float()\n",
        "        X = F.relu(self.fc1_l(X))\n",
        "        X = F.relu(self.fc2_l(X))\n",
        "        X = F.softmax(self.final(X))\n",
        "        return X\n",
        "\n",
        "    def get_actions(self,state):\n",
        "        state = T.tensor(state).float()\n",
        "        probs = self.forward(state)\n",
        "        highest_prob_action = np.random.choice(self.n_actions, p=np.squeeze(probs.detach().numpy()))\n",
        "        log_prob = T.log(probs.squeeze(0)[highest_prob_action])\n",
        "        return highest_prob_action, log_prob\n",
        "\n",
        "    def update_policy(self,log_probs,rewards):\n",
        "        discounted_rewards = []\n",
        "        policy_gradients = []\n",
        "        for t in range(len(rewards)):\n",
        "            Gt = 0\n",
        "            p = 0\n",
        "            for i in range(t,len(rewards)):\n",
        "                Gt+=(self.gamma**p)*rewards[i]\n",
        "                p+=1\n",
        "            discounted_rewards.append(Gt)\n",
        "        discounted_rewards = torch.tensor(discounted_rewards)\n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) \n",
        "        for prob,Gt in zip(log_probs,discounted_rewards):\n",
        "            policy_gradients.append(-prob*Gt)\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_gradients = T.stack(policy_gradients).sum()\n",
        "\n",
        "        policy_gradients.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1utgIgh6lra",
        "outputId": "90d01356-62f7-40f9-dc0f-5d35a2f5e23c"
      },
      "source": [
        "agent = Policy_Network(0.0001,(4,),2)\n",
        "episodes = 10000\n",
        "gamma = 0.999\n",
        "\n",
        "scores, eps_history = [],[]\n",
        "for i in range(episodes):\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    done = False\n",
        "    while not done:\n",
        "        action,log_prob = agent.get_actions(state)\n",
        "        state_,reward,done,info = env.step(action)\n",
        "        state = state_\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "    agent.update_policy(log_probs,rewards)\n",
        "    scores.append(sum(rewards))\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "    if i%200==0:\n",
        "        print(\"episode : {} | score : {} | average score :{} | gamma : {}\".format(\n",
        "                                                                            i,sum(rewards),avg_score,agent.gamma))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode : 0 | score : 19.0 | average score :19.0 | gamma : 0.999\n",
            "episode : 200 | score : 53.0 | average score :25.92 | gamma : 0.999\n",
            "episode : 400 | score : 10.0 | average score :27.08 | gamma : 0.999\n",
            "episode : 600 | score : 12.0 | average score :28.79 | gamma : 0.999\n",
            "episode : 800 | score : 14.0 | average score :31.83 | gamma : 0.999\n",
            "episode : 1000 | score : 47.0 | average score :30.11 | gamma : 0.999\n",
            "episode : 1200 | score : 28.0 | average score :29.29 | gamma : 0.999\n",
            "episode : 1400 | score : 24.0 | average score :35.42 | gamma : 0.999\n",
            "episode : 1600 | score : 32.0 | average score :37.94 | gamma : 0.999\n",
            "episode : 1800 | score : 55.0 | average score :39.83 | gamma : 0.999\n",
            "episode : 2000 | score : 17.0 | average score :44.96 | gamma : 0.999\n",
            "episode : 2200 | score : 55.0 | average score :47.77 | gamma : 0.999\n",
            "episode : 2400 | score : 35.0 | average score :47.72 | gamma : 0.999\n",
            "episode : 2600 | score : 88.0 | average score :52.45 | gamma : 0.999\n",
            "episode : 2800 | score : 20.0 | average score :55.26 | gamma : 0.999\n",
            "episode : 3000 | score : 32.0 | average score :57.39 | gamma : 0.999\n",
            "episode : 3200 | score : 30.0 | average score :59.43 | gamma : 0.999\n",
            "episode : 3400 | score : 42.0 | average score :63.84 | gamma : 0.999\n",
            "episode : 3600 | score : 200.0 | average score :79.29 | gamma : 0.999\n",
            "episode : 3800 | score : 129.0 | average score :88.18 | gamma : 0.999\n",
            "episode : 4000 | score : 89.0 | average score :109.18 | gamma : 0.999\n",
            "episode : 4200 | score : 183.0 | average score :124.56 | gamma : 0.999\n",
            "episode : 4400 | score : 78.0 | average score :140.25 | gamma : 0.999\n",
            "episode : 4600 | score : 200.0 | average score :144.65 | gamma : 0.999\n",
            "episode : 4800 | score : 172.0 | average score :165.15 | gamma : 0.999\n",
            "episode : 5000 | score : 156.0 | average score :160.2 | gamma : 0.999\n",
            "episode : 5200 | score : 134.0 | average score :170.59 | gamma : 0.999\n",
            "episode : 5400 | score : 200.0 | average score :179.14 | gamma : 0.999\n",
            "episode : 5600 | score : 160.0 | average score :173.25 | gamma : 0.999\n",
            "episode : 5800 | score : 200.0 | average score :173.51 | gamma : 0.999\n",
            "episode : 6000 | score : 166.0 | average score :177.9 | gamma : 0.999\n",
            "episode : 6200 | score : 153.0 | average score :177.58 | gamma : 0.999\n",
            "episode : 6400 | score : 195.0 | average score :180.94 | gamma : 0.999\n",
            "episode : 6600 | score : 102.0 | average score :185.86 | gamma : 0.999\n",
            "episode : 6800 | score : 200.0 | average score :183.95 | gamma : 0.999\n",
            "episode : 7000 | score : 200.0 | average score :191.34 | gamma : 0.999\n",
            "episode : 7200 | score : 200.0 | average score :191.33 | gamma : 0.999\n",
            "episode : 7400 | score : 200.0 | average score :191.13 | gamma : 0.999\n",
            "episode : 7600 | score : 153.0 | average score :181.84 | gamma : 0.999\n",
            "episode : 7800 | score : 195.0 | average score :189.87 | gamma : 0.999\n",
            "episode : 8000 | score : 200.0 | average score :189.57 | gamma : 0.999\n",
            "episode : 8200 | score : 200.0 | average score :191.36 | gamma : 0.999\n",
            "episode : 8400 | score : 200.0 | average score :190.13 | gamma : 0.999\n",
            "episode : 8600 | score : 200.0 | average score :194.6 | gamma : 0.999\n",
            "episode : 8800 | score : 200.0 | average score :180.84 | gamma : 0.999\n",
            "episode : 9000 | score : 200.0 | average score :170.32 | gamma : 0.999\n",
            "episode : 9200 | score : 200.0 | average score :198.56 | gamma : 0.999\n",
            "episode : 9400 | score : 156.0 | average score :192.86 | gamma : 0.999\n",
            "episode : 9600 | score : 200.0 | average score :190.15 | gamma : 0.999\n",
            "episode : 9800 | score : 200.0 | average score :192.7 | gamma : 0.999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W65so-aY9xCe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}